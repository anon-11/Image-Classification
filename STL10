import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (SeparableConv2D, BatchNormalization, MaxPooling2D,
                                     GlobalAveragePooling2D, Dense, Dropout)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

# Load the STL-10 dataset
stl10_dataset, dataset_info = tfds.load("stl10", with_info=True, as_supervised=True)
train_dataset = stl10_dataset['train']
test_dataset = stl10_dataset['test']

AUTOTUNE = tf.data.AUTOTUNE

# Preprocessing with normalization and one-hot encoding
def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    label = tf.one_hot(label, depth=10)
    return image, label

train_dataset = train_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)
test_dataset = test_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)

# Data augmentation
def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_crop(image, size=[96, 96, 3])
    image = tf.image.random_brightness(image, max_delta=0.3)
    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)
    image = tf.image.random_saturation(image, lower=0.7, upper=1.3)
    image = tf.image.random_hue(image, max_delta=0.05)
    return image, label

train_dataset = train_dataset.map(augment, num_parallel_calls=AUTOTUNE)

# Prefetch and batch
train_dataset = train_dataset.shuffle(1000).batch(64).prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.batch(64).prefetch(buffer_size=AUTOTUNE)

# Define the model
from tensorflow.keras import regularizers

model = Sequential([
    SeparableConv2D(64, (3, 3), activation='relu', padding='same', 
                    input_shape=(96, 96, 3)),
    BatchNormalization(),
    SeparableConv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Add regularizers separately in Dense layers
for layer in model.layers:
    if isinstance(layer, Dense):
        layer.kernel_regularizer = regularizers.l2(1e-4)

# Optimizer and callbacks
optimizer = Adam(learning_rate=1e-3)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.2, min_lr=1e-6, verbose=1)

# Compile the model
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Training loop
epochs = 100
history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, 
                    callbacks=[lr_scheduler], verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(test_dataset)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Save accuracy and loss data
import pandas as pd
data = {
    "Epoch": list(range(1, len(history.history['accuracy']) + 1)),
    "Training Accuracy": history.history['accuracy'],
    "Validation Accuracy": history.history['val_accuracy'],
}
df = pd.DataFrame(data)
df.to_excel("stl10_accuracy_data.xlsx", index=False)
print("Accuracy and loss data saved to 'stl10_accuracy_data.xlsx'")

# Plot accuracy and loss
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(history.history['accuracy']) + 1), history.history['accuracy'], label='Training Accuracy')
plt.plot(range(1, len(history.history['val_accuracy']) + 1), history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

