import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (SeparableConv2D, BatchNormalization, MaxPooling2D,
                                     Flatten, Dense, Dropout, GlobalAveragePooling2D)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Input

from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load the STL-10 dataset
stl10_dataset, dataset_info = tfds.load("stl10", with_info=True, as_supervised=True)
train_dataset = stl10_dataset['train']
test_dataset = stl10_dataset['test']

AUTOTUNE = tf.data.AUTOTUNE

# Preprocessing with normalization and one-hot encoding
def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    label = tf.one_hot(label, depth=10)
    return image, label

train_dataset = train_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)
test_dataset = test_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)

# Data augmentation
def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_crop(image, size=[96, 96, 3])
    image = tf.image.random_brightness(image, max_delta=0.2)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    return image, label

train_dataset = train_dataset.map(augment, num_parallel_calls=AUTOTUNE)

# Prefetch and batch
train_dataset = train_dataset.shuffle(1000).batch(64).prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.batch(64).prefetch(buffer_size=AUTOTUNE)

# Define the model
model = Sequential([
        Input(shape=(96, 96, 3)),  # Explicit input layer
        SeparableConv2D(1024, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),

        SeparableConv2D(1024, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),

        SeparableConv2D(1024, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),

        SeparableConv2D(1024, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),

        GlobalAveragePooling2D(),
        Dense(1024, activation='relu', kernel_regularizer=l2(1e-4)),
        Dropout(0.5),
        Dense(1024, activation='relu', kernel_regularizer=l2(1e-4)),
        Dropout(0.5),
        Dense(10, activation='softmax')
    ])

# Optimizer and learning rate scheduler
optimizer = Adam(learning_rate=1e-4)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)

# Compile the model
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Training loop
epochs = 500
history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, 
                    callbacks=[lr_scheduler], verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(test_dataset)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Save accuracy and loss data
data = {
    "Epoch": list(range(1, epochs + 1)),
    "Training Accuracy": history.history['accuracy'],
    "Validation Accuracy": history.history['val_accuracy'],
    "Training Loss": history.history['loss'],
    "Validation Loss": history.history['val_loss']
}
df = pd.DataFrame(data)
df.to_excel("stl10_accuracy_data.xlsx", index=False)
print("Accuracy and loss data saved to 'stl10_accuracy_data.xlsx'")

# Plot accuracy and loss
plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), history.history['accuracy'], label='Training Accuracy')
plt.plot(range(1, epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), history.history['loss'], label='Training Loss')
plt.plot(range(1, epochs + 1), history.history['val_loss'], label='Validation Loss')
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
